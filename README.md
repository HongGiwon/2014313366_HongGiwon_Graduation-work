# 2014313366_HongGiwon_Graduation-work

-학습 데이터는 나무위키의 데이터 베이스를 NamuMarkAST(https://github.com/MerHS/biryo) 를 이용하여 추출해 html 태그, 특수 문자 등을 제거하여 정제함
-디시인사이드의 글을 크롤링하여 가져온 데이터 역시 학습에 사용

-데이터 전처리 과정

1. parser로 dump json -> txt
2. editplus(텍스트 에디터)로 html 태그, 리다이렉트, 상위 문서 등의 위키 문법 텍스트 제거
4. refine.java로 특수문자 제거(정규화 기반)
5. 텍스트 에디터로 한글, control 문자, 숫자, 공백을 제외한 모든 문자 제거. ([^\u0000-\u001f\u0020\u0030-\u0039\u1100-\u11ff\u3130-\u318f\ua960-\ua97f\uac00-\ud7af\ud7b0-\ud7ff])
5. 텍스트 에디터로 [0-99][.] 같은 일부 위키 문법 텍스트 추가 제거
6. 문장 부호 (. ? !)를 기반으로 문장 단위로 나눔
7. 토크나이징(konlpy-twitter)

(약 8시간 정도 소요됨)

-학습 과정
1. gensim 파이썬 패키지를 사용하여 word2vec 모델 생성
2. 모델 학습 (나무위키 데이터, 디시인 사이드 데이터) 
-- 모든 corpus를 메모리에 올릴수 x
-> 메모리 효율적으로 하기 위해 iteration을 위한 클래스(한번에 하나의 파일을 읽어서 문장을 yield) 만듦
3. 모델 저장
4. 새로운 단어 및 문장 온라인 학습